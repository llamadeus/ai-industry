{
 "cells": [
  {
   "cell_type": "code",
   "id": "3db9a370-3f21-49b6-a6be-e414f03d1e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:05:06.710141Z",
     "start_time": "2025-03-09T15:05:04.619009Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# Notebook setup: run this before everything\n",
    "# ============================================================\n",
    "# -- Copied from lecture\n",
    "%load_ext autoreload\n",
    "%config IPCompleter.greedy=True\n",
    "%autoreload 1\n",
    "%aimport util\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from util import util\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Control figure size\n",
    "interactive_figures = False\n",
    "if interactive_figures:\n",
    "    # Normal behavior\n",
    "    %matplotlib widget\n",
    "    figsize=(9, 3)\n",
    "else:\n",
    "    # PDF export behavior\n",
    "    figsize=(14, 4)\n",
    "\n",
    "# Load datasets\n",
    "raw_data = util.load_dataset('7_gecco2019_train_water_quality.csv')\n",
    "val_data = util.load_dataset('8_gecco2019_valid_water_quality.csv')\n",
    "test_data = util.load_dataset('6_gecco2019_test_water_quality.csv')"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "2212816f",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models\n",
    "Gaussian Mixture Models (GMM) are a popular unsupervised learning algorithm that can be used to model the distribution of a dataset. In the context of anomaly detection, GMM can be used to find clusters of normal data points and identify anomalies. GMMs describe the distribution via a weighted sum of Gaussian components.\n",
    "\n",
    "GMMs assume, that data is generated by the following probabilistic model:\n",
    "$$\n",
    "X_Z,\n",
    "$$\n",
    "where both $Z$ and $X_Z$ are random variables. $Z$ is a latent variable that represents the component of the data, while $X_Z$ is the observed data. The latent variable $Z$ is assumed to be generated by a probability distribution $p(Z)$, while $X_Z$ follows a multivariate Gaussian distribution.\n",
    "\n",
    "In mathematical terms, a GMM is a probability distribution that can be represented as:\n",
    "$$\n",
    "g(x, \\mu, \\Sigma, \\tau) = \\sum_{k=1}^{n} \\tau_{k} \\mathcal{f}(x, \\mu_{k}, \\Sigma_{k}),\n",
    "$$\n",
    "where $\\tau$ is a vector of weights, $\\mu$ is a vector of means, $\\Sigma$ is a covariance matrix, and $\\mathcal{f}$ is the Gaussian probability density function."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "In order to use GMM for anomaly detection, we first need to make sure, that our training data is free from missing values and anomalies. As seen before, a linear interpolation approach yields the best results. Therefore, we will interpolate missing values using this method. Then, we have to apply a sliding window approach using the `aggregation_length` parameter explained above and aggregate the data into windows. This makes sure that we capture temporal correlations between data points and additionally removes noise in the data. The final step of our preprocessing pipeline is to standardize the data."
   ],
   "id": "8404782eff05530c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:05:06.935233Z",
     "start_time": "2025-03-09T15:05:06.713067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Interpolate missing values\n",
    "gmm_data_df = util.impute_missing_values(raw_data)\n",
    "\n",
    "# Interpolate all values where Event is True\n",
    "gmm_data_df.loc[gmm_data_df['Event'] == True, util.get_feature_columns(gmm_data_df)] = np.nan\n",
    "gmm_data_df = util.impute_missing_values(gmm_data_df)\n",
    "\n",
    "# Apply sliding window and aggregate\n",
    "gmm_data_df = util.apply_sliding_window_and_aggregate(gmm_data_df)\n",
    "\n",
    "# Identify the features to be used for GMM\n",
    "gmm_features = util.get_feature_columns(gmm_data_df)\n",
    "\n",
    "# Standardize the data (GMM assumes normally distributed data)\n",
    "gmm_scaler = StandardScaler()\n",
    "gmm_data_df[gmm_features] = gmm_scaler.fit_transform(gmm_data_df[gmm_features])\n",
    "\n",
    "print(gmm_data_df.head())\n",
    "\n",
    "# Preprocess validation data\n",
    "gmm_val_data_df = util.impute_missing_values(val_data)\n",
    "gmm_val_data_df = util.apply_sliding_window_and_aggregate(gmm_val_data_df)\n",
    "\n",
    "# Standardize the data (KDE assumes normally distributed data)\n",
    "gmm_val_data_df[gmm_features] = gmm_scaler.transform(gmm_val_data_df[gmm_features])"
   ],
   "id": "188a4c7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     window_0  window_1  window_2  window_3  window_4  \\\n",
      "Time                                                                    \n",
      "2017-07-01 00:09:00 -1.232383  1.187793 -0.392840 -0.809330 -1.697608   \n",
      "2017-07-01 00:10:00 -1.243568  1.158432 -0.359859 -0.372582 -1.686283   \n",
      "2017-07-01 00:11:00 -1.232383  1.099867 -0.352313 -0.622491 -1.709695   \n",
      "2017-07-01 00:12:00 -1.232383  1.099867 -0.342251 -0.723540 -1.694760   \n",
      "2017-07-01 00:13:00 -1.232383  1.129229 -0.337220 -0.138949 -1.710721   \n",
      "\n",
      "                     window_5  window_6  window_7  window_8  window_9  ...  \\\n",
      "Time                                                                   ...   \n",
      "2017-07-01 00:09:00 -2.181413 -1.243574  1.158442 -0.359853 -0.372584  ...   \n",
      "2017-07-01 00:10:00 -2.209374 -1.232388  1.099877 -0.352306 -0.622493  ...   \n",
      "2017-07-01 00:11:00 -2.195118 -1.232388  1.099877 -0.342244 -0.723542  ...   \n",
      "2017-07-01 00:12:00 -2.215171 -1.232388  1.129239 -0.337213 -0.138951  ...   \n",
      "2017-07-01 00:13:00 -2.233105 -1.232388  1.129239 -0.317369 -0.748296  ...   \n",
      "\n",
      "                     window_pH_var  window_Cond_mean  window_Cond_var  \\\n",
      "Time                                                                    \n",
      "2017-07-01 00:09:00      -0.020377         -0.346344        -0.023483   \n",
      "2017-07-01 00:10:00      -0.020380         -0.339765        -0.023422   \n",
      "2017-07-01 00:11:00      -0.020381         -0.331148        -0.023286   \n",
      "2017-07-01 00:12:00      -0.020384         -0.323377        -0.023179   \n",
      "2017-07-01 00:13:00      -0.020382         -0.315003        -0.023040   \n",
      "\n",
      "                     window_Turb_mean  window_Turb_var  window_SAC_mean  \\\n",
      "Time                                                                      \n",
      "2017-07-01 00:09:00         -0.781869        -0.032793        -1.699838   \n",
      "2017-07-01 00:10:00         -0.773363        -0.033236        -1.697388   \n",
      "2017-07-01 00:11:00         -0.766849        -0.033611        -1.695684   \n",
      "2017-07-01 00:12:00         -0.787979        -0.033602        -1.691228   \n",
      "2017-07-01 00:13:00         -0.797876        -0.033835        -1.689731   \n",
      "\n",
      "                     window_SAC_var  window_PFM_mean  window_PFM_var  Event  \n",
      "Time                                                                         \n",
      "2017-07-01 00:09:00       -0.020878        -2.225460       -0.491586    0.0  \n",
      "2017-07-01 00:10:00       -0.020667        -2.223620       -0.496276    0.0  \n",
      "2017-07-01 00:11:00       -0.020589        -2.228924       -0.465726    0.0  \n",
      "2017-07-01 00:12:00       -0.019677        -2.228047       -0.474174    0.0  \n",
      "2017-07-01 00:13:00       -0.019680        -2.233472       -0.435718    0.0  \n",
      "\n",
      "[5 rows x 73 columns]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Determine the number of Gaussians\n",
    "Next, we need to determine the number of Gaussians to use for our GMM. We can do this using the Bayesian Information Criterion (BIC) or the elbow method. The BIC is a measure of the model's goodness of fit, while the elbow method is a visual tool that helps us determine the optimal number of Gaussians. In our case, we will use the BIC method."
   ],
   "id": "3689a93e482d8f6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:08:24.692127Z",
     "start_time": "2025-03-09T15:05:07.041927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select only relevant features (mean and variance).\n",
    "X = gmm_data_df[gmm_features]\n",
    "X_val = gmm_val_data_df[gmm_features]\n",
    "\n",
    "# Fit GMM and determine optimal K using BIC.\n",
    "lowest_bic = np.inf\n",
    "best_k = None\n",
    "\n",
    "# Try GMMs with 1 to 10 components.\n",
    "for k in range(1, 11):\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)\n",
    "    gmm.fit(X)\n",
    "\n",
    "    bic = gmm.bic(X_val)\n",
    "\n",
    "    # Do we have a better model?\n",
    "    if bic < lowest_bic:\n",
    "        lowest_bic = bic\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Best GMM with {best_k} components, BIC: {lowest_bic:.2f}\")"
   ],
   "id": "3913ab42c989abf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GMM with 4 components, BIC: 5556969130.96\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the GMM\n",
    "Next, we train the GMM with the optimal number of components. Additionally, we compute the log likelihood scores for each data point. This score is a measure of how likely a data point is to be generated by the Gaussian distribution. Higher scores indicate a higher likelihood of being generated by a Gaussian distribution."
   ],
   "id": "96e79a9b5b63a9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:08:33.707139Z",
     "start_time": "2025-03-09T15:08:24.727863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train GMM with best K.\n",
    "final_gmm = GaussianMixture(n_components=best_k, covariance_type='full', random_state=42)\n",
    "final_gmm.fit(X)\n",
    "\n",
    "# Compute likelihood scores for the training data.\n",
    "log_likelihood = final_gmm.score_samples(X)"
   ],
   "id": "bbc0d8a56786f859",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Threshold Optimization\n",
    "Now, we need to define a threshold to separate normal data from anomalous data. We will use a simple threshold optimization approach. First, we define the percentiles to test. Then, we compute precision, recall, and F1-score for each percentile. These are the preferred metrics when working with big class imbalances. Finally, we select the percentile with the highest F1-score."
   ],
   "id": "63b81c200a2cb339"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:08:34.013272Z",
     "start_time": "2025-03-09T15:08:33.710447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute likelihood scores for the training data.\n",
    "log_likelihood_val = final_gmm.score_samples(X_val)\n",
    "\n",
    "# Define percentiles to test.\n",
    "percentiles = np.arange(0.1, 2.1, 0.1)\n",
    "\n",
    "# For storing the results.\n",
    "results = []\n",
    "\n",
    "for p in percentiles:\n",
    "    # Get predictions and threshold\n",
    "    y_pred, threshold = util.get_predictions_from_log_likelihood(log_likelihood_val, p)\n",
    "\n",
    "    # Compute performance\n",
    "    f1, precision, recall = util.compute_model_performance(y_pred, gmm_val_data_df['Event'])\n",
    "\n",
    "    # Store results.\n",
    "    results.append((p, threshold, precision, recall, f1))\n",
    "\n",
    "# Convert to DataFrame for better visualization.\n",
    "df_results = pd.DataFrame(results, columns=['Percentile', 'Threshold', 'Precision', 'Recall', 'F1-score'])\n",
    "\n",
    "# Display results.\n",
    "gmm_best_percentile = df_results.loc[df_results['F1-score'].idxmax()]\n",
    "print(df_results)\n",
    "print(f\"Best GMM model with percentile {gmm_best_percentile['Percentile']} and threshold {gmm_best_percentile['Threshold']} achieves an F1-score of {gmm_best_percentile['F1-score']}\")"
   ],
   "id": "4a8eb81a94a2b7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Percentile     Threshold  Precision    Recall  F1-score\n",
      "0          0.1 -2.400630e+07   0.581818  0.096677  0.165803\n",
      "1          0.2 -7.842527e+06   0.390909  0.129909  0.195011\n",
      "2          0.3 -4.660417e+06   0.345455  0.172205  0.229839\n",
      "3          0.4 -3.218652e+06   0.401826  0.265861  0.320000\n",
      "4          0.5 -1.416382e+06   0.434307  0.359517  0.393388\n",
      "5          0.6 -7.554792e+05   0.428571  0.425982  0.427273\n",
      "6          0.7 -1.533860e+05   0.450521  0.522659  0.483916\n",
      "7          0.8 -4.419262e+04   0.420091  0.555891  0.478544\n",
      "8          0.9 -4.188588e+04   0.375254  0.558912  0.449029\n",
      "9          1.0 -3.977271e+04   0.337591  0.558912  0.420933\n",
      "10         1.1 -3.771574e+04   0.306799  0.558912  0.396146\n",
      "11         1.2 -3.571500e+04   0.281583  0.558912  0.374494\n",
      "12         1.3 -3.380550e+04   0.259831  0.558912  0.354746\n",
      "13         1.4 -3.215464e+04   0.241199  0.558912  0.336976\n",
      "14         1.5 -3.034745e+04   0.225061  0.558912  0.320902\n",
      "15         1.6 -2.865872e+04   0.211187  0.558912  0.306545\n",
      "16         1.7 -2.698904e+04   0.199785  0.561934  0.294770\n",
      "17         1.8 -2.534138e+04   0.188641  0.561934  0.282460\n",
      "18         1.9 -2.371877e+04   0.178674  0.561934  0.271137\n",
      "19         2.0 -2.220862e+04   0.170776  0.564955  0.262272\n",
      "Best GMM model with percentile 0.7000000000000001 and threshold -153386.04003278978 achieves an F1-score of 0.48391608391608393\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Compute Number of Anomalies\n",
    "Now, we compute the number of anomalies for the identified threshold."
   ],
   "id": "28d623974cc404e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:08:34.037552Z",
     "start_time": "2025-03-09T15:08:34.033073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute number of anomalies\n",
    "threshold = np.percentile(log_likelihood, gmm_best_percentile['Percentile'])\n",
    "anomalies = log_likelihood < threshold\n",
    "\n",
    "print(f\"Anomaly threshold: {threshold:.2f}\")\n",
    "print(f\"Number of anomalies: {np.sum(anomalies)}\")\n",
    "\n",
    "# Add anomaly labels to DataFrame\n",
    "gmm_data_df['Anomaly_Score'] = log_likelihood\n",
    "gmm_data_df['Anomaly'] = anomalies\n",
    "\n",
    "print(gmm_data_df['Anomaly_Score'].head())"
   ],
   "id": "c6b1efbbc83604d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 76.12\n",
      "Number of anomalies: 928\n",
      "Time\n",
      "2017-07-01 00:09:00    142.628872\n",
      "2017-07-01 00:10:00    144.959728\n",
      "2017-07-01 00:11:00    146.009770\n",
      "2017-07-01 00:12:00    141.062214\n",
      "2017-07-01 00:13:00    138.712673\n",
      "Name: Anomaly_Score, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix and Classification Report\n",
    "To measure the performance of our anomaly detection model on the training data, we can use the confusion matrix and classification report."
   ],
   "id": "b7b4c747c7765ea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:08:34.087629Z",
     "start_time": "2025-03-09T15:08:34.056834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert Boolean to integers for evaluation\n",
    "y_true = gmm_data_df['Event'].astype(int)  # Actual contamination events\n",
    "y_pred = gmm_data_df['Anomaly'].astype(int)  # Detected anomalies\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))"
   ],
   "id": "84a73c1a5f87a962",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[131230    912]\n",
      " [   313     16]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00    132142\n",
      "           1       0.02      0.05      0.03       329\n",
      "\n",
      "    accuracy                           0.99    132471\n",
      "   macro avg       0.51      0.52      0.51    132471\n",
      "weighted avg       1.00      0.99      0.99    132471\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The GMM performed well on the training data with an overall accuracy of 99%, but this high accuracy is largely driven by the class imbalance that favors normal samples. The model correctly classified nearly all normal cases (TN = 131,230) with a precision of 1.00, resulting in relatively few false positives (FP = 912). However, when it comes to anomalies, the performance is much weaker. The model only detected 16 out of 329 anomalies (recall = 0.05), meaning that it missed 313 true anomalies. Moreover, the anomaly precision of 0.02 indicates that almost all of the flagged anomalies were false alarms, and the F1-score of 0.03 further highlights the severe imbalance between precision and recall.",
   "id": "5d4c69b280db4070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance on Test Data\n",
    "The lack of generalization can be seen when we test the trained GMM on the test data. First, we have to perform the same preprocessing steps as with the training data. Afterwards, we can apply the GMM to the test data and compute the confusion matrix and classification report."
   ],
   "id": "69f53cfca7dd602a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T15:08:34.223709Z",
     "start_time": "2025-03-09T15:08:34.123385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess test data\n",
    "gmm_test_data_df = util.impute_missing_values(test_data)\n",
    "gmm_test_data_df = util.apply_sliding_window_and_aggregate(gmm_test_data_df)\n",
    "\n",
    "# Standardize the data (GMM assumes normally distributed data)\n",
    "gmm_test_data_df[gmm_features] = gmm_scaler.transform(gmm_test_data_df[gmm_features])\n",
    "\n",
    "# Compute scores for test data\n",
    "X_test = gmm_test_data_df[gmm_features]\n",
    "log_likelihood_test = final_gmm.score_samples(X_test)  # Higher is more normal, lower is more anomalous\n",
    "\n",
    "threshold_test = np.percentile(log_likelihood_test, gmm_best_percentile['Percentile'])\n",
    "anomalies_test = log_likelihood_test < threshold_test\n",
    "\n",
    "# Add anomaly labels to DataFrame\n",
    "gmm_test_data_df['Anomaly_Score'] = log_likelihood_test\n",
    "gmm_test_data_df['Anomaly'] = anomalies_test\n",
    "\n",
    "# Convert Boolean to integers for evaluation\n",
    "y_true = gmm_test_data_df['Event'].astype(int)  # Actual contamination events\n",
    "y_pred = gmm_test_data_df['Anomaly'].astype(int)  # Detected anomalies\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))"
   ],
   "id": "6f9bf144b7a2983e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[31195   147]\n",
      " [  224    75]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     31342\n",
      "           1       0.34      0.25      0.29       299\n",
      "\n",
      "    accuracy                           0.99     31641\n",
      "   macro avg       0.67      0.62      0.64     31641\n",
      "weighted avg       0.99      0.99      0.99     31641\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On the test data, the performance of the GMM deteriorated notably compared to the training results. For normal samples (class 0), the model still performed very well with a 99% accuracy. However, for anomalies (class 1), the recall was only 25%, meaning that only one-quarter of the actual anomalies were correctly detected, while 224 anomalies were missed. Moreover, the anomaly precision was 34%, indicating that a significant number of the detected anomalies were false positives. The F1-score for anomalies of 0.29 further highlights the model’s struggle to effectively balance precision and recall. These results underscore the need for improved generalization techniques, such as adjusting the detection threshold, retraining with a more balanced dataset, or exploring alternative anomaly detection methods.",
   "id": "2dade5d75f7347c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection_for_contamination_detect-I_ojTiUE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
