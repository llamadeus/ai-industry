{
 "cells": [
  {
   "cell_type": "code",
   "id": "3db9a370-3f21-49b6-a6be-e414f03d1e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T22:35:29.516230Z",
     "start_time": "2025-03-07T22:35:29.365093Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# Notebook setup: run this before everything\n",
    "# ============================================================\n",
    "# -- Copied from lecture\n",
    "%load_ext autoreload\n",
    "%config IPCompleter.greedy=True\n",
    "%autoreload 1\n",
    "%aimport util\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from util import util\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Control figure size\n",
    "interactive_figures = False\n",
    "if interactive_figures:\n",
    "    # Normal behavior\n",
    "    %matplotlib widget\n",
    "    figsize=(9, 3)\n",
    "else:\n",
    "    # PDF export behavior\n",
    "    figsize=(14, 4)\n",
    "\n",
    "raw_data = util.load_dataset('7_gecco2019_train_water_quality.csv')\n",
    "val_data = util.load_dataset('8_gecco2019_valid_water_quality.csv')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "2212816f",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models\n",
    "Gaussian Mixture Models (GMM) are a popular unsupervised learning algorithm that can be used to model the distribution of a dataset. In the context of anomaly detection, GMM can be used to find clusters of normal data points and identify anomalies. GMMs describe the distribution via a weighted sum of Gaussian components.\n",
    "\n",
    "GMMs assume, that data is generated by the following probabilistic model:\n",
    "$$\n",
    "X_Z,\n",
    "$$\n",
    "where both $Z$ and $X_Z$ are random variables. $Z$ is a latent variable that represents the component of the data, while $X_Z$ is the observed data. The latent variable $Z$ is assumed to be generated by a probability distribution $p(Z)$, while $X_Z$ follows a multivariate Gaussian distribution.\n",
    "\n",
    "In mathematical terms, a GMM is a probability distribution that can be represented as:\n",
    "$$\n",
    "g(x, \\mu, \\Sigma, \\tau) = \\sum_{k=1}^{n} \\tau_{k} \\mathcal{f}(x, \\mu_{k}, \\Sigma_{k}),\n",
    "$$\n",
    "where $\\tau$ is a vector of weights, $\\mu$ is a vector of means, $\\Sigma$ is a covariance matrix, and $\\mathcal{f}$ is the Gaussian probability density function."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "In order to use GMM for anomaly detection, we first need to make sure, that our data is free from missing values. As seen before, a linear interpolation approach yields the best results. Therefore, we will interpolate missing values using this method. Then, we have to apply a sliding window approach using the `aggregation_length` parameter explained above and aggregate the data into windows. This makes sure that we capture temporal correlations between data points and additionally removes noise in the data. The final step of our preprocessing pipeline is to standardize the data."
   ],
   "id": "8404782eff05530c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T22:35:37.335200Z",
     "start_time": "2025-03-07T22:35:37.100294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the data (interpolating missing values and applying sliding window)\n",
    "gmm_data_df = util.impute_missing_values(raw_data)\n",
    "gmm_data_df = util.apply_sliding_window_and_aggregate(gmm_data_df)\n",
    "\n",
    "# Identify the features to be used for GMM\n",
    "gmm_features = util.get_feature_columns(gmm_data_df)\n",
    "\n",
    "# Standardize the data (GMM assumes normally distributed data)\n",
    "gmm_scaler = StandardScaler()\n",
    "gmm_data_df[gmm_features] = gmm_scaler.fit_transform(gmm_data_df[gmm_features])\n",
    "\n",
    "print(gmm_data_df.head())\n",
    "\n",
    "# Preprocess validation data\n",
    "gmm_val_data_df = util.impute_missing_values(val_data)\n",
    "gmm_val_data_df = util.apply_sliding_window_and_aggregate(gmm_val_data_df)\n",
    "\n",
    "# Standardize the data (KDE assumes normally distributed data)\n",
    "gmm_val_data_df[gmm_features] = gmm_scaler.transform(gmm_val_data_df[gmm_features])"
   ],
   "id": "188a4c7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     window_0  window_1  window_2  window_3  window_4  \\\n",
      "Time                                                                    \n",
      "2017-07-01 00:09:00 -1.231517  1.060557 -0.382443 -0.408179 -1.684389   \n",
      "2017-07-01 00:10:00 -1.242696  1.034397 -0.350244 -0.192011 -1.673169   \n",
      "2017-07-01 00:11:00 -1.231517  0.982218 -0.342876 -0.315704 -1.696363   \n",
      "2017-07-01 00:12:00 -1.231517  0.982218 -0.333053 -0.365718 -1.681567   \n",
      "2017-07-01 00:13:00 -1.231517  1.008378 -0.328141 -0.076374 -1.697380   \n",
      "\n",
      "                     window_5  window_6  window_7  window_8  window_9  ...  \\\n",
      "Time                                                                   ...   \n",
      "2017-07-01 00:09:00 -2.181417 -1.242701  1.034405 -0.350237 -0.192011  ...   \n",
      "2017-07-01 00:10:00 -2.209378 -1.231522  0.982226 -0.342870 -0.315704  ...   \n",
      "2017-07-01 00:11:00 -2.195122 -1.231522  0.982226 -0.333047 -0.365719  ...   \n",
      "2017-07-01 00:12:00 -2.215175 -1.231522  1.008386 -0.328135 -0.076374  ...   \n",
      "2017-07-01 00:13:00 -2.233109 -1.231522  1.008386 -0.308761 -0.377970  ...   \n",
      "\n",
      "                     window_pH_var  window_Cond_mean  window_Cond_var  \\\n",
      "Time                                                                    \n",
      "2017-07-01 00:09:00      -0.024241         -0.343533        -0.026828   \n",
      "2017-07-01 00:10:00      -0.024244         -0.336985        -0.026778   \n",
      "2017-07-01 00:11:00      -0.024245         -0.328408        -0.026665   \n",
      "2017-07-01 00:12:00      -0.024247         -0.320675        -0.026576   \n",
      "2017-07-01 00:13:00      -0.024246         -0.312340        -0.026460   \n",
      "\n",
      "                     window_Turb_mean  window_Turb_var  window_SAC_mean  \\\n",
      "Time                                                                      \n",
      "2017-07-01 00:09:00         -0.723460        -0.023907        -1.697050   \n",
      "2017-07-01 00:10:00         -0.715781        -0.023939        -1.694608   \n",
      "2017-07-01 00:11:00         -0.709900        -0.023967        -1.692909   \n",
      "2017-07-01 00:12:00         -0.728976        -0.023966        -1.688467   \n",
      "2017-07-01 00:13:00         -0.737910        -0.023983        -1.686975   \n",
      "\n",
      "                     window_SAC_var  window_PFM_mean  window_PFM_var  Event  \n",
      "Time                                                                         \n",
      "2017-07-01 00:09:00       -0.046138        -2.225470       -0.492060    0.0  \n",
      "2017-07-01 00:10:00       -0.045997        -2.223629       -0.496749    0.0  \n",
      "2017-07-01 00:11:00       -0.045945        -2.228933       -0.466199    0.0  \n",
      "2017-07-01 00:12:00       -0.045336        -2.228056       -0.474647    0.0  \n",
      "2017-07-01 00:13:00       -0.045338        -2.233482       -0.436190    0.0  \n",
      "\n",
      "[5 rows x 73 columns]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Determine the number of Gaussians\n",
    "Next, we need to determine the number of Gaussians to use for our GMM. We can do this using the Bayesian Information Criterion (BIC) or the elbow method. The BIC is a measure of the model's goodness of fit, while the elbow method is a visual tool that helps us determine the optimal number of Gaussians. In our case, we will use the BIC method."
   ],
   "id": "3689a93e482d8f6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T22:38:15.534752Z",
     "start_time": "2025-03-07T22:35:39.463545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select only relevant features (mean and variance).\n",
    "X = gmm_data_df[gmm_features]\n",
    "X_val = gmm_val_data_df[gmm_features]\n",
    "\n",
    "# Fit GMM and determine optimal K using BIC.\n",
    "lowest_bic = np.inf\n",
    "best_k = None\n",
    "\n",
    "# Try GMMs with 1 to 10 components.\n",
    "for k in range(1, 11):\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)\n",
    "    gmm.fit(X)\n",
    "\n",
    "    bic = gmm.bic(X_val)\n",
    "\n",
    "    # Do we have a better model?\n",
    "    if bic < lowest_bic:\n",
    "        lowest_bic = bic\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Best GMM with {best_k} components, BIC: {lowest_bic:.2f}\")"
   ],
   "id": "3913ab42c989abf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best GMM with 3 components, BIC: 580398095.80\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the GMM\n",
    "Next, we train the GMM with the optimal number of components. Additionally, we compute the log likelihood scores for each data point. This score is a measure of how likely a data point is to be generated by the Gaussian distribution. Higher scores indicate a higher likelihood of being generated by a Gaussian distribution."
   ],
   "id": "96e79a9b5b63a9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:04:53.925677Z",
     "start_time": "2025-03-07T23:04:47.685767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train GMM with best K.\n",
    "best_k = 3\n",
    "final_gmm = GaussianMixture(n_components=best_k, covariance_type='full', random_state=42)\n",
    "final_gmm.fit(X)\n",
    "\n",
    "# Compute likelihood scores for the training data.\n",
    "log_likelihood = final_gmm.score_samples(X)"
   ],
   "id": "bbc0d8a56786f859",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Threshold Optimization\n",
    "Now, we need to define a threshold to separate normal data from anomalous data. We will use a simple threshold optimization approach. First, we define the percentiles to test. Then, we compute precision, recall, and F1-score for each percentile. These are the preferred metrics when working with big class imbalances. Finally, we select the percentile with the highest F1-score."
   ],
   "id": "63b81c200a2cb339"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:05:01.047619Z",
     "start_time": "2025-03-07T23:05:00.714840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute likelihood scores for the training data.\n",
    "log_likelihood_val = final_gmm.score_samples(X_val)\n",
    "\n",
    "# Define percentiles to test.\n",
    "percentiles = np.arange(0.1, 2.1, 0.1)\n",
    "\n",
    "# For storing the results.\n",
    "results = []\n",
    "\n",
    "for p in percentiles:\n",
    "    # Get predictions and threshold\n",
    "    y_pred, threshold = util.get_predictions_from_log_likelihood(log_likelihood_val, p)\n",
    "\n",
    "    # Compute performance\n",
    "    f1, precision, recall = util.compute_model_performance(y_pred, gmm_val_data_df['Event'])\n",
    "\n",
    "    # Store results.\n",
    "    results.append((p, threshold, precision, recall, f1))\n",
    "\n",
    "# Convert to DataFrame for better visualization.\n",
    "df_results = pd.DataFrame(results, columns=['Percentile', 'Threshold', 'Precision', 'Recall', 'F1-score'])\n",
    "\n",
    "# Display results.\n",
    "gmm_best_percentile = df_results.loc[df_results['F1-score'].idxmax()]\n",
    "print(df_results)\n",
    "print(f\"Best GMM model with percentile {gmm_best_percentile['Percentile']} and threshold {gmm_best_percentile['Threshold']} achieves an F1-score of {gmm_best_percentile['F1-score']}\")"
   ],
   "id": "4a8eb81a94a2b7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Percentile     Threshold  Precision    Recall  F1-score\n",
      "0          0.1 -42684.072814   0.000000  0.000000  0.000000\n",
      "1          0.2 -40559.058023   0.000000  0.000000  0.000000\n",
      "2          0.3 -38488.776691   0.000000  0.000000  0.000000\n",
      "3          0.4 -36473.229550   0.000000  0.000000  0.000000\n",
      "4          0.5 -34512.419886   0.000000  0.000000  0.000000\n",
      "5          0.6 -32606.343682   0.000000  0.000000  0.000000\n",
      "6          0.7 -30755.000937   0.000000  0.000000  0.000000\n",
      "7          0.8 -28958.393113   0.000000  0.000000  0.000000\n",
      "8          0.9 -27216.522036   0.000000  0.000000  0.000000\n",
      "9          1.0 -25529.384419   0.000000  0.000000  0.000000\n",
      "10         1.1 -23896.980261   0.000000  0.000000  0.000000\n",
      "11         1.2 -22319.311754   0.000000  0.000000  0.000000\n",
      "12         1.3 -20796.379264   0.000000  0.000000  0.000000\n",
      "13         1.4 -19328.180234   0.000000  0.000000  0.000000\n",
      "14         1.5 -17914.714663   0.000000  0.000000  0.000000\n",
      "15         1.6 -16555.985474   0.000000  0.000000  0.000000\n",
      "16         1.7 -15251.991571   0.000000  0.000000  0.000000\n",
      "17         1.8 -14002.731128   0.016227  0.048338  0.024298\n",
      "18         1.9 -12808.204143   0.028818  0.090634  0.043732\n",
      "19         2.0 -11668.414272   0.027397  0.090634  0.042076\n",
      "Best GMM model with percentile 1.9000000000000001 and threshold -12808.204143493718 achieves an F1-score of 0.043731778425655975\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Compute Number of Anomalies\n",
    "Now, we compute the number of anomalies for the identified threshold."
   ],
   "id": "28d623974cc404e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:05:13.003144Z",
     "start_time": "2025-03-07T23:05:12.997994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute number of anomalies\n",
    "threshold = np.percentile(log_likelihood, gmm_best_percentile['Percentile'])\n",
    "anomalies = log_likelihood < threshold\n",
    "\n",
    "print(f\"Anomaly threshold: {threshold:.2f}\")\n",
    "print(f\"Number of anomalies: {np.sum(anomalies)}\")\n",
    "\n",
    "# Add anomaly labels to DataFrame\n",
    "gmm_data_df['Anomaly_Score'] = log_likelihood\n",
    "gmm_data_df['Anomaly'] = anomalies\n",
    "\n",
    "print(gmm_data_df['Anomaly_Score'].head())"
   ],
   "id": "c6b1efbbc83604d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: 99.43\n",
      "Number of anomalies: 2517\n",
      "Time\n",
      "2017-07-01 00:09:00    153.153172\n",
      "2017-07-01 00:10:00    155.260781\n",
      "2017-07-01 00:11:00    157.076251\n",
      "2017-07-01 00:12:00    152.309765\n",
      "2017-07-01 00:13:00    149.886448\n",
      "Name: Anomaly_Score, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix and Classification Report\n",
    "To measure the performance of our anomaly detection model on the training data, we can use the confusion matrix and classification report."
   ],
   "id": "b7b4c747c7765ea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:05:16.763821Z",
     "start_time": "2025-03-07T23:05:16.732736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert Boolean to integers for evaluation\n",
    "y_true = gmm_data_df['Event'].astype(int)  # Actual contamination events\n",
    "y_pred = gmm_data_df['Anomaly'].astype(int)  # Detected anomalies\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))"
   ],
   "id": "84a73c1a5f87a962",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[129919   2223]\n",
      " [    35    294]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    132142\n",
      "           1       0.12      0.89      0.21       329\n",
      "\n",
      "    accuracy                           0.98    132471\n",
      "   macro avg       0.56      0.94      0.60    132471\n",
      "weighted avg       1.00      0.98      0.99    132471\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The GMM performed well on the training data with a high overall accuracy of 98%, but this is primarily due to the class imbalance that favors normal samples. The model correctly classified nearly all normal cases (TN = 129,919) with a precision of 1.00. This indicates relatively few false positives (FP = 2,223). However, while it detected 89% of actual anomalies (recall = 0.89), it still missed 11% (FN = 35). The precision for anomalies (0.12) suggests that only a small portion of the detected anomalies were true contaminations, meaning there are still many false alarms. The F1-score of 0.21 shows an imbalance between precision and recall. We can conclude that the model is moderately effective at detecting anomalies in the training set, but further improvements may be needed for generalization to unseen data.",
   "id": "5d4c69b280db4070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance on Test Data\n",
    "The lack of generalization can be seen when we test the trained GMM on the test data. First, we have to perform the same preprocessing steps as with the training data. Afterwards, we can apply the GMM to the test data and compute the confusion matrix and classification report."
   ],
   "id": "69f53cfca7dd602a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:06:39.240251Z",
     "start_time": "2025-03-07T23:06:39.108815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load test data\n",
    "test_data = util.load_dataset('6_gecco2019_test_water_quality.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "gmm_test_data_df = util.impute_missing_values(test_data)\n",
    "gmm_test_data_df = util.apply_sliding_window_and_aggregate(gmm_test_data_df)\n",
    "\n",
    "# Standardize the data (GMM assumes normally distributed data)\n",
    "gmm_test_data_df[gmm_features] = gmm_scaler.transform(gmm_test_data_df[gmm_features])\n",
    "\n",
    "# Compute scores for test data\n",
    "X_test = gmm_test_data_df[gmm_features]\n",
    "log_likelihood_test = final_gmm.score_samples(X_test)  # Higher is more normal, lower is more anomalous\n",
    "\n",
    "threshold_test = np.percentile(log_likelihood_test, gmm_best_percentile['Percentile'])\n",
    "anomalies_test = log_likelihood_test < threshold_test\n",
    "\n",
    "# Add anomaly labels to DataFrame\n",
    "gmm_test_data_df['Anomaly_Score'] = log_likelihood_test\n",
    "gmm_test_data_df['Anomaly'] = anomalies_test\n",
    "\n",
    "# Convert Boolean to integers for evaluation\n",
    "y_true = gmm_test_data_df['Event'].astype(int)  # Actual contamination events\n",
    "y_pred = gmm_test_data_df['Anomaly'].astype(int)  # Detected anomalies\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))"
   ],
   "id": "6f9bf144b7a2983e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[30788   554]\n",
      " [  251    48]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     31342\n",
      "           1       0.08      0.16      0.11       299\n",
      "\n",
      "    accuracy                           0.97     31641\n",
      "   macro avg       0.54      0.57      0.55     31641\n",
      "weighted avg       0.98      0.97      0.98     31641\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On the test data, the performance of the GMM dropped significantly, with anomaly recall dropping to 16%. The model failed to detect most contaminants (FN = 251). While normal samples were still well classified (98% accuracy for class 0), anomaly precision was only 8%, indicating a high false positive rate. The F1-score of 0.11 confirms that the model struggles to generalize, probably due to overfitting on training data. These results suggest that the model needs better generalization techniques, such as adjusting the threshold, retraining with a more balanced dataset, or considering alternative anomaly detection methods.",
   "id": "2dade5d75f7347c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection_for_contamination_detect-I_ojTiUE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
