{
 "cells": [
  {
   "cell_type": "code",
   "id": "3db9a370-3f21-49b6-a6be-e414f03d1e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T22:17:20.219501Z",
     "start_time": "2025-03-07T22:17:19.945465Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# Notebook setup: run this before everything\n",
    "# ============================================================\n",
    "# -- Copied from lecture\n",
    "%load_ext autoreload\n",
    "%config IPCompleter.greedy=True\n",
    "%autoreload 1\n",
    "%aimport util\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from util import util\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Control figure size\n",
    "interactive_figures = False\n",
    "if interactive_figures:\n",
    "    # Normal behavior\n",
    "    %matplotlib widget\n",
    "    figsize=(9, 3)\n",
    "else:\n",
    "    # PDF export behavior\n",
    "    figsize=(14, 4)\n",
    "\n",
    "raw_data = util.load_dataset('7_gecco2019_train_water_quality.csv')\n",
    "raw_data = util.impute_missing_values(raw_data)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "2212816f",
   "metadata": {},
   "source": [
    "# Multivariate Kernel Density Estimation\n",
    "The first approach presented in the lecture is **Kernel Density Estimation**\n",
    "\n",
    "In order to employ **KDE**, we need to determine the optimal **Kernel Function** and **Bandwidth**.\n",
    "Since we have multiple columns, we cannot use the Rule Of Thumb for the latter. Therefore, we need to optimize the following term according to the lecture:\n",
    "$$\n",
    "\\mathop{\\arg\\max}_{h} \\mathbb{E}_{x \\sim f(x), \\bar{x} \\sim f(x)}\\left[ L(h, x, \\bar{x})\\right]\n",
    "$$\n",
    "where\n",
    "- $$\n",
    "L(h, x, \\bar{x}) = \\prod_{i=1}^m \\hat{f}(x_i, \\bar{x}_i, h)\n",
    "$$\n",
    "- $\\hat{f}$ is the density estimator (which outputs a probability)\n",
    "- $\\bar{x}$ the training set\n",
    "\n",
    "according to the lecture."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "KDE is a non-parametric method, meaning that it does not make any assumptions about the shape and the distribution of the data. Therefore, we need to preprocess the data before applying KDE. First, we have to make sure that our data is free from missing values. As seen before, a linear interpolation approach yields the best results. Therefore, we will interpolate missing values using this method. Then, we have to apply a sliding window approach using the `aggregation_length` parameter explained before and aggregate the data into windows. This makes sure that we capture temporal correlations between data points and additionally removes noise in the data. The final step of our preprocessing pipeline is to standardize the data."
   ],
   "id": "8404782eff05530c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T22:17:20.378707Z",
     "start_time": "2025-03-07T22:17:20.224908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocess the data (interpolating missing values and applying sliding window)\n",
    "kde_data_df = util.impute_missing_values(raw_data)\n",
    "kde_data_df = util.apply_sliding_window_and_aggregate(kde_data_df)\n",
    "\n",
    "# Identify the features to be used for KDE\n",
    "kde_features = util.get_feature_columns(kde_data_df)\n",
    "\n",
    "# Standardize the data (KDE assumes normally distributed data)\n",
    "kde_scaler = StandardScaler()\n",
    "kde_data_df[kde_features] = kde_scaler.fit_transform(kde_data_df[kde_features])\n",
    "\n",
    "print(kde_data_df.head())"
   ],
   "id": "188a4c7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     window_0  window_1  window_2  window_3  window_4  \\\n",
      "Time                                                                    \n",
      "2017-07-01 00:09:00 -1.231517  1.060557 -0.382443 -0.408179 -1.684389   \n",
      "2017-07-01 00:10:00 -1.242696  1.034397 -0.350244 -0.192011 -1.673169   \n",
      "2017-07-01 00:11:00 -1.231517  0.982218 -0.342876 -0.315704 -1.696363   \n",
      "2017-07-01 00:12:00 -1.231517  0.982218 -0.333053 -0.365718 -1.681567   \n",
      "2017-07-01 00:13:00 -1.231517  1.008378 -0.328141 -0.076374 -1.697380   \n",
      "\n",
      "                     window_5  window_6  window_7  window_8  window_9  ...  \\\n",
      "Time                                                                   ...   \n",
      "2017-07-01 00:09:00 -2.181417 -1.242701  1.034405 -0.350237 -0.192011  ...   \n",
      "2017-07-01 00:10:00 -2.209378 -1.231522  0.982226 -0.342870 -0.315704  ...   \n",
      "2017-07-01 00:11:00 -2.195122 -1.231522  0.982226 -0.333047 -0.365719  ...   \n",
      "2017-07-01 00:12:00 -2.215175 -1.231522  1.008386 -0.328135 -0.076374  ...   \n",
      "2017-07-01 00:13:00 -2.233109 -1.231522  1.008386 -0.308761 -0.377970  ...   \n",
      "\n",
      "                     window_pH_var  window_Cond_mean  window_Cond_var  \\\n",
      "Time                                                                    \n",
      "2017-07-01 00:09:00      -0.024241         -0.343533        -0.026828   \n",
      "2017-07-01 00:10:00      -0.024244         -0.336985        -0.026778   \n",
      "2017-07-01 00:11:00      -0.024245         -0.328408        -0.026665   \n",
      "2017-07-01 00:12:00      -0.024247         -0.320675        -0.026576   \n",
      "2017-07-01 00:13:00      -0.024246         -0.312340        -0.026460   \n",
      "\n",
      "                     window_Turb_mean  window_Turb_var  window_SAC_mean  \\\n",
      "Time                                                                      \n",
      "2017-07-01 00:09:00         -0.723460        -0.023907        -1.697050   \n",
      "2017-07-01 00:10:00         -0.715781        -0.023939        -1.694608   \n",
      "2017-07-01 00:11:00         -0.709900        -0.023967        -1.692909   \n",
      "2017-07-01 00:12:00         -0.728976        -0.023966        -1.688467   \n",
      "2017-07-01 00:13:00         -0.737910        -0.023983        -1.686975   \n",
      "\n",
      "                     window_SAC_var  window_PFM_mean  window_PFM_var  Event  \n",
      "Time                                                                         \n",
      "2017-07-01 00:09:00       -0.046138        -2.225470       -0.492060    0.0  \n",
      "2017-07-01 00:10:00       -0.045997        -2.223629       -0.496749    0.0  \n",
      "2017-07-01 00:11:00       -0.045945        -2.228933       -0.466199    0.0  \n",
      "2017-07-01 00:12:00       -0.045336        -2.228056       -0.474647    0.0  \n",
      "2017-07-01 00:13:00       -0.045338        -2.233482       -0.436190    0.0  \n",
      "\n",
      "[5 rows x 73 columns]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bandwidth Choice in Multivariate KDE\n",
    "The bandwidth parameter $h$ is a hyperparameter that controls the smoothness of the estimated density. It has to be chosen such that it maximizes the likelihood of the data:\n",
    "\n",
    "$$\n",
    "argmax_h \\mathbb{E}_{x \\sim f(x), \\bar{x} \\sim f(x)}\\left[ L(h, x, \\bar{x})\\right],\n",
    "$$\n",
    "\n",
    "where $f(x)$ is the true distribution function and $L(h, x, \\bar{x)$ is the log-likelihood function.\n",
    "\n",
    " In the lecture, we discussed the following methods to determine the optimal bandwidth:\n",
    "- The rule of thumb method: This method uses a rule of thumb to estimate the optimal bandwidth based on the data. It involves calculating the standard deviation of the data and using it as the bandwidth. However, this method is not always reliable, as it can be sensitive to the scale of the data.\n",
    "- Cross-validation: This method involves splitting the data into training and validation sets and using the validation set to estimate the optimal bandwidth. The model is then trained on the training set and evaluated on the validation set. This method is more robust than the rule of thumb method, as it can handle non-normal data and avoid overfitting.\n",
    "- Grid search: This method involves searching over a grid of possible bandwidths and selecting the one that gives the best performance. This method is computationally expensive, but it can be useful when the data is not normally distributed.\n",
    "\n",
    "In our case, we will use a grid search to determine the optimal bandwidth. We will use the F1-score to evaluate the performance of the KDE model using a given bandwidth. The grid search will be performed using the `sklearn.model_selection.GridSearchCV` class. As we're working with time-series data, we will use a `sklearn.model_selection.TimeSeriesSplit` cross-validation strategy to split the data into training and validation sets.\n",
    "\n",
    "The downside of the grid search strategy is that it is computationally very expensive, especially for large datasets, as it runs a cross-validation for each possible bandwidth value."
   ],
   "id": "3689a93e482d8f6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T22:33:52.429166Z",
     "start_time": "2025-03-07T22:17:20.499495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select only features columns.\n",
    "X = kde_data_df[kde_features]\n",
    "\n",
    "# Define a reasonable range of bandwidths\n",
    "bandwidths = np.linspace(0.7, 0.8, 5)\n",
    "\n",
    "# Create a TimeSeriesSplit instance to ensure training data precedes test data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Set up GridSearchCV with verbose output to monitor progress; n_jobs=-1 uses all cores\n",
    "grid = GridSearchCV(KernelDensity(kernel='gaussian'), {'bandwidth': bandwidths}, cv=tscv, verbose=3, n_jobs=-1)\n",
    "\n",
    "# Run the grid search to fit the KDE model and determine the best bandwidth\n",
    "grid.fit(X)\n",
    "\n",
    "best_bw = grid.best_params_['bandwidth']\n",
    "print(\"Best bandwidth:\", grid.best_params_['bandwidth'])\n",
    "print(\"Best log-likelihood score:\", grid.best_score_)"
   ],
   "id": "3913ab42c989abf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV 4/5] END ............bandwidth=0.725;, score=-1221040.668 total time= 8.0min\n",
      "[CV 1/5] END ..............bandwidth=0.8;, score=-1400469.650 total time= 2.6min\n",
      "Best bandwidth: 0.7\n",
      "Best log-likelihood score: -1827092.5103767694\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train the KDE\n",
    "Next, we train the KDE with the optimal bandwidth. Additionally, we compute the log likelihood scores for each data point. This score is a measure of how likely a data point is to be generated by the underlying distribution. Higher scores indicate a higher likelihood of being generated by a Gaussian distribution.\n",
    "\n",
    "Due to the high dimensionality of the data, training and evaluating the KDE model suffer from the \"curse of dimensionality\". This means that the model becomes increasingly difficult to train and evaluate as the number of dimensions increases."
   ],
   "id": "96e79a9b5b63a9fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:34:44.210756Z",
     "start_time": "2025-03-07T22:33:52.455092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select only features columns.\n",
    "X = kde_data_df[kde_features]\n",
    "\n",
    "# Train KDE with best bandwidth.\n",
    "final_kde = KernelDensity(kernel='gaussian', bandwidth=best_bw)\n",
    "final_kde.fit(X)\n",
    "\n",
    "# Compute likelihood scores for the training data.\n",
    "log_likelihood = final_kde.score_samples(X)"
   ],
   "id": "bbc0d8a56786f859",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END ............bandwidth=0.725;, score=-1292972.402 total time= 2.4min\n",
      "[CV 2/5] END .............bandwidth=0.75;, score=-2182875.470 total time= 4.4min\n",
      "[CV 3/5] END ............bandwidth=0.775;, score=-2675668.799 total time= 5.0min\n",
      "[CV 2/5] END ............bandwidth=0.725;, score=-2204778.200 total time= 4.4min\n",
      "[CV 4/5] END .............bandwidth=0.75;, score=-1265856.338 total time= 8.2min\n",
      "[CV 5/5] END ..............bandwidth=0.7;, score=-1623409.327 total time= 9.1min\n",
      "[CV 2/5] END ..............bandwidth=0.8;, score=-2153508.216 total time= 3.8min\n",
      "[CV 3/5] END ..............bandwidth=0.7;, score=-2852069.041 total time= 4.6min\n",
      "[CV 5/5] END .............bandwidth=0.75;, score=-1664421.965 total time= 9.0min\n",
      "[CV 2/5] END ..............bandwidth=0.7;, score=-2227191.085 total time= 4.3min\n",
      "[CV 3/5] END .............bandwidth=0.75;, score=-2726704.334 total time= 4.8min\n",
      "[CV 3/5] END ..............bandwidth=0.8;, score=-2639169.914 total time= 4.6min\n",
      "[CV 3/5] END ............bandwidth=0.725;, score=-2784282.746 total time= 4.7min\n",
      "[CV 1/5] END ............bandwidth=0.775;, score=-1365054.653 total time= 2.5min\n",
      "[CV 4/5] END ............bandwidth=0.775;, score=-1308230.502 total time= 7.5min\n",
      "[CV 4/5] END ..............bandwidth=0.7;, score=-1175862.629 total time= 8.0min\n",
      "[CV 5/5] END ............bandwidth=0.775;, score=-1681945.150 total time= 7.6min\n",
      "[CV 5/5] END ............bandwidth=0.725;, score=-1639264.879 total time= 9.2min\n",
      "[CV 4/5] END ..............bandwidth=0.8;, score=-1350317.033 total time= 6.4min\n",
      "[CV 1/5] END ..............bandwidth=0.7;, score=-1256930.469 total time= 2.4min\n",
      "[CV 1/5] END .............bandwidth=0.75;, score=-1328957.264 total time= 2.4min\n",
      "[CV 2/5] END ............bandwidth=0.775;, score=-2166805.469 total time= 4.5min\n",
      "[CV 5/5] END ..............bandwidth=0.8;, score=-1708221.645 total time= 7.1min\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Threshold Optimization\n",
    "Now, we need to define a threshold to separate normal data from anomalous data. We will use a simple threshold optimization approach using the validation set. First, we define the percentiles to test. Then, we compute precision, recall, and F1-score for each percentile. These are the preferred metrics when working with big class imbalances. Finally, we select the percentile with the highest F1-score."
   ],
   "id": "63b81c200a2cb339"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:48:55.112908Z",
     "start_time": "2025-03-07T23:34:44.229160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "val_data = util.load_dataset('8_gecco2019_valid_water_quality.csv')\n",
    "\n",
    "# Preprocess validation data\n",
    "kde_val_data_df = util.impute_missing_values(val_data)\n",
    "kde_val_data_df = util.apply_sliding_window_and_aggregate(kde_val_data_df)\n",
    "\n",
    "# Standardize the data (KDE assumes normally distributed data)\n",
    "kde_val_data_df[kde_features] = kde_scaler.transform(kde_val_data_df[kde_features])\n",
    "\n",
    "# Compute scores for validation data\n",
    "log_likelihood_val = final_kde.score_samples(kde_val_data_df[kde_features])\n",
    "\n",
    "# Define percentiles to test.\n",
    "percentiles = np.arange(0.1, 2.1, 0.1)\n",
    "\n",
    "# For storing the results.\n",
    "results = []\n",
    "\n",
    "for p in percentiles:\n",
    "    # Get predictions and threshold\n",
    "    y_pred, threshold = util.get_predictions_from_log_likelihood(log_likelihood_val, p)\n",
    "\n",
    "    # Compute performance\n",
    "    f1, precision, recall = util.compute_model_performance(y_pred, kde_val_data_df['Event'])\n",
    "\n",
    "    # Store results.\n",
    "    results.append((p, threshold, precision, recall, f1))\n",
    "\n",
    "# Convert to DataFrame for better visualization.\n",
    "df_results = pd.DataFrame(results, columns=['Percentile', 'Threshold', 'Precision', 'Recall', 'F1-score'])\n",
    "\n",
    "# Display results.\n",
    "kde_best_percentile = df_results.loc[df_results['F1-score'].idxmax()]\n",
    "print(df_results)\n",
    "print(f\"Best KDE model with percentile {kde_best_percentile['Percentile']} and threshold {kde_best_percentile['Threshold']} achieves an F1-score of {kde_best_percentile['F1-score']}\")"
   ],
   "id": "4a8eb81a94a2b7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Percentile     Threshold  Precision    Recall  F1-score\n",
      "0          0.1 -75948.705540   0.181818  0.030211  0.051813\n",
      "1          0.2 -71218.840030   0.100000  0.033233  0.049887\n",
      "2          0.3 -66921.981164   0.084848  0.042296  0.056452\n",
      "3          0.4 -62831.763739   0.082192  0.054381  0.065455\n",
      "4          0.5 -58837.337355   0.065693  0.054381  0.059504\n",
      "5          0.6 -55024.673594   0.057751  0.057402  0.057576\n",
      "6          0.7 -51718.769490   0.049479  0.057402  0.053147\n",
      "7          0.8 -48605.275273   0.045662  0.060423  0.052016\n",
      "8          0.9 -45486.712633   0.040568  0.060423  0.048544\n",
      "9          1.0 -42791.386100   0.038321  0.063444  0.047782\n",
      "10         1.1 -40338.049568   0.041459  0.075529  0.053533\n",
      "11         1.2 -38085.564301   0.041096  0.081571  0.054656\n",
      "12         1.3 -36084.714872   0.039326  0.084592  0.053691\n",
      "13         1.4 -34365.271081   0.039113  0.090634  0.054645\n",
      "14         1.5 -32837.884047   0.038929  0.096677  0.055507\n",
      "15         1.6 -31527.391661   0.036530  0.096677  0.053024\n",
      "16         1.7 -29987.252290   0.035446  0.099698  0.052298\n",
      "17         1.8 -28330.984560   0.033469  0.099698  0.050114\n",
      "18         1.9 -27001.631448   0.060519  0.190332  0.091837\n",
      "19         2.0 -25916.017836   0.057534  0.190332  0.088359\n",
      "Best KDE model with percentile 1.9000000000000001 and threshold -27001.631447588672 achieves an F1-score of 0.09183673469387756\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Compute number of anomalies\n",
    "Now, we compute the number of anomalies for the identified threshold."
   ],
   "id": "28d623974cc404e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:48:55.144647Z",
     "start_time": "2025-03-07T23:48:55.138933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute number of anomalies\n",
    "anomalies, threshold = util.get_predictions_from_log_likelihood(log_likelihood, kde_best_percentile['Percentile'])\n",
    "\n",
    "print(f\"Anomaly threshold: {threshold:.2f}\")\n",
    "print(f\"Number of anomalies: {np.sum(anomalies)}\")\n",
    "\n",
    "# Add anomaly labels to DataFrame\n",
    "kde_data_df['Anomaly_Score'] = log_likelihood\n",
    "kde_data_df['Anomaly'] = anomalies\n",
    "\n",
    "print(kde_data_df['Anomaly_Score'].head())"
   ],
   "id": "c6b1efbbc83604d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly threshold: -53.46\n",
      "Number of anomalies: 2517\n",
      "Time\n",
      "2017-07-01 00:09:00   -48.804299\n",
      "2017-07-01 00:10:00   -48.801081\n",
      "2017-07-01 00:11:00   -48.808160\n",
      "2017-07-01 00:12:00   -48.840240\n",
      "2017-07-01 00:13:00   -48.888673\n",
      "Name: Anomaly_Score, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Confusion Matrix and Classification Report\n",
    "To measure the performance of our anomaly detection model on the training data, we can use the confusion matrix and classification report."
   ],
   "id": "b7b4c747c7765ea9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:48:55.188599Z",
     "start_time": "2025-03-07T23:48:55.158354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert Boolean to integers for evaluation\n",
    "y_true = kde_data_df['Event'].astype(int)  # Actual contamination events\n",
    "y_pred = kde_data_df['Anomaly'].astype(int)  # Detected anomalies\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))"
   ],
   "id": "84a73c1a5f87a962",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[129879   2263]\n",
      " [    75    254]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99    132142\n",
      "           1       0.10      0.77      0.18       329\n",
      "\n",
      "    accuracy                           0.98    132471\n",
      "   macro avg       0.55      0.88      0.58    132471\n",
      "weighted avg       1.00      0.98      0.99    132471\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model achieves a very high overall accuracy of 98%, driven primarily by its excellent performance on the dominant class (class 0), where it correctly identifies almost all cases (98% recall, nearly 100% precision). However, this high accuracy masks a significant weakness in detecting the minority class (class 1). For class 1, although the recall is moderately high at 77% (meaning that most actual positives are captured), the precision is extremely low at just 10%, indicating that a large number of false positives are being flagged. This imbalance suggests that while the KDE model is very effective at recognizing the majority class, it struggles to reliably identify the minority class, due to the imbalanced nature of the training data.",
   "id": "5d4c69b280db4070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Performance on Test Data\n",
    "Finally, we evaluate the model's performance on the test data. This is an important step that allows us to assess how well the model generalizes to unseen data. First, we have to perform the same preprocessing steps as with the training data. In the last step, we can apply the KDE to the test data and compute the confusion matrix and classification report."
   ],
   "id": "69f53cfca7dd602a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T23:56:23.389363Z",
     "start_time": "2025-03-07T23:48:55.208659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load test data\n",
    "test_data = util.load_dataset('6_gecco2019_test_water_quality.csv')\n",
    "\n",
    "# Preprocess test data\n",
    "kde_test_data_df = util.impute_missing_values(test_data)\n",
    "kde_test_data_df = util.apply_sliding_window_and_aggregate(kde_test_data_df)\n",
    "\n",
    "# Standardize the data (KDE assumes normally distributed data)\n",
    "kde_test_data_df[kde_features] = kde_scaler.transform(kde_test_data_df[kde_features])\n",
    "\n",
    "# Compute scores for test data\n",
    "X_test = kde_test_data_df[kde_features]\n",
    "log_likelihood_test = final_kde.score_samples(X_test)  # Higher is more normal, lower is more anomalous\n",
    "\n",
    "# Get predictions and threshold\n",
    "anomalies_test, threshold_test = util.get_predictions_from_log_likelihood(log_likelihood_test, kde_best_percentile['Percentile'])\n",
    "\n",
    "# Add anomaly labels to DataFrame\n",
    "kde_test_data_df['Anomaly_Score'] = log_likelihood_test\n",
    "kde_test_data_df['Anomaly'] = anomalies_test\n",
    "\n",
    "# Convert Boolean to integers for evaluation\n",
    "y_true = kde_test_data_df['Event'].astype(int)  # Actual contamination events\n",
    "y_pred = kde_test_data_df['Anomaly'].astype(int)  # Detected anomalies\n",
    "\n",
    "# Print performance metrics\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_true, y_pred))"
   ],
   "id": "6f9bf144b7a2983e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[30867   475]\n",
      " [  172   127]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99     31342\n",
      "           1       0.21      0.42      0.28       299\n",
      "\n",
      "    accuracy                           0.98     31641\n",
      "   macro avg       0.60      0.70      0.64     31641\n",
      "weighted avg       0.99      0.98      0.98     31641\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "On the test data, the model maintains a high overall accuracy of 98% and continues to perform well for the majority class (class 0) with an accuracy of 99% and a recall of 98%. However, compared to the training results, the performance for the minority class (Class 1) has changed: while the recall in training was higher at 77%, the recall in testing drops to 42%, indicating that the model is now missing more true positives. Conversely, the accuracy for Class 1 improves from 10% in the training set to 21% in the test set, meaning that the model is more likely to correctly predict Class 1 with the unseen data. Although the F1 score has improved, it is still very low at 0.28. These differences indicate a possible problem of overfitting or that the model's ability to generalize to the minority class is limited, as its sensitivity to the test data has decreased despite a slight improvement in prediction accuracy for the minority class.",
   "id": "2dade5d75f7347c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection_for_contamination_detect-I_ojTiUE-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
